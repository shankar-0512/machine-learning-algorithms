{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmrtjvl49qJt",
        "outputId": "d2a0a6f0-96fb-4dfe-cc9b-92795a07202c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.1.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow\n",
        "! pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0bfIDdV19wHx"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 1\n",
        "tf.random.set_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define the DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, memory_size=1000000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = []\n",
        "        self.memory_size = memory_size\n",
        "        self.model = QNetwork(state_dim, action_dim)\n",
        "        self.target_model = QNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        if len(self.memory) >= self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # def act(self, state):\n",
        "    #     if np.random.rand() < self.epsilon:\n",
        "    #         return np.random.choice(self.action_dim)\n",
        "    #     state_tensor = torch.FloatTensor(state)\n",
        "    #     q_values = self.model(state_tensor)\n",
        "    #     return torch.argmax(q_values).item()\n",
        "\n",
        "    def select_action(self, state, explore=True):\n",
        "        if explore and np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.action_dim)\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        q_values = self.model(state_tensor)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        samples = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        states_tensor = torch.FloatTensor(states)\n",
        "        next_states_tensor = torch.FloatTensor(next_states)\n",
        "        actions_tensor = torch.LongTensor(actions).unsqueeze(1)\n",
        "        rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "        dones_tensor = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "        # Compute the Q-values for the current state and the next state\n",
        "        q_values = self.model(states_tensor).gather(1, actions_tensor)\n",
        "        next_q_values = self.target_model(next_states_tensor).detach()\n",
        "        max_next_q_values = next_q_values.max(1)[0].unsqueeze(1)\n",
        "        target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * max_next_q_values\n",
        "\n",
        "        # Compute the loss and backpropagate\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(1)\n",
        "\n",
        "# Set hyperparameters\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "memory_size = 1000000\n",
        "batch_size = 64\n",
        "num_episodes = 640\n",
        "\n",
        "# Create the DQN agent\n",
        "agent = DQNAgent(state_dim, action_dim, learning_rate, gamma, epsilon, epsilon_min, epsilon_decay, memory_size, batch_size)\n",
        "\n",
        "# Train the agent\n",
        "total_rewards_train = []\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose an action\n",
        "        action = agent.select_action(state, explore=True)\n",
        "\n",
        "        # Take a step\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the agent's memory\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        # Update the Q-value function\n",
        "        agent.replay()\n",
        "\n",
        "        # Update the target network\n",
        "        agent.update_target_model()\n",
        "\n",
        "        # Update the state and the total reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    # Append the total reward to the list\n",
        "    total_rewards_train.append(total_reward)\n",
        "    # Decay the exploration rate\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "# Test the agent\n",
        "num_test_episodes = 100\n",
        "total_rewards_test = []\n",
        "for episode in range(num_test_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose the best action\n",
        "        action = agent.select_action(state, explore=False)\n",
        "\n",
        "        # Take a step\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the total reward\n",
        "        total_reward += reward\n",
        "\n",
        "        # Update the state\n",
        "        state = next_state\n",
        "\n",
        "    # Append the total reward to the list\n",
        "    total_rewards_test.append(total_reward)\n",
        "\n",
        "# Print the average reward over the test episodes\n",
        "avg_reward = sum(total_rewards_test) / num_test_episodes\n",
        "print(\"Average Reward: {:.2f}\".format(avg_reward))\n",
        "print(\"\")\n",
        "\n",
        "# Define the figure with two subplots\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "# Plot average rewards during training\n",
        "axs[0].set_title(\"Lunar Lander Training\", fontsize=20)\n",
        "axs[0].set_xlabel(\"Episode\", fontsize=16)\n",
        "axs[0].set_ylabel(\"Total Reward\", fontsize=16)\n",
        "axs[0].tick_params(axis=\"both\", labelsize=14)\n",
        "axs[0].grid(linestyle='--', alpha=0.7)\n",
        "axs[0].plot(range(len(total_rewards_train)), total_rewards_train, color='#1f77b4', linewidth=2)\n",
        "\n",
        "# Plot total rewards during testing\n",
        "axs[1].set_title(\"Lunar Lander Testing\", fontsize=20)\n",
        "axs[1].set_xlabel('Episode', fontsize=16)\n",
        "axs[1].set_ylabel('Total Reward', fontsize=16)\n",
        "axs[1].tick_params(axis='both', labelsize=14)\n",
        "axs[1].grid(linestyle='--', alpha=0.7)\n",
        "axs[1].plot(range(len(total_rewards_test)), total_rewards_test, color='#ff7f0e', linewidth=2)\n",
        "\n",
        "# Add space between the subplots\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Add background color\n",
        "fig.patch.set_facecolor('#F5F5F5')\n",
        "axs[0].set_facecolor('#FFFFFF')\n",
        "axs[1].set_facecolor('#FFFFFF')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jgCq0nlr92H7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}